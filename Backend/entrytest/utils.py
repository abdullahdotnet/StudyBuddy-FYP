import os
import PyPDF2
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import CharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain_groq import ChatGroq
from reportlab.lib.pagesizes import letter
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer
from reportlab.lib.enums import TA_JUSTIFY
from django.conf import settings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import HumanMessage
import json
import glob
import pandas as pd

def extract_text_from_pdf(pdf_path):
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        text = ""
        for page in reader.pages:
            text += page.extract_text()
    return text

def get_pdf_files_from_folder(folder_name):
    entrytest_dir = os.path.dirname(os.path.abspath(__file__))
    folder_path = os.path.join(entrytest_dir, folder_name)  # Join with the entrytest directory

    pdf_files = []
    try:
        for file in os.listdir(folder_path):
            if file.lower().endswith('.pdf'):
                pdf_files.append(os.path.join(folder_path, file))
    except FileNotFoundError:
        print(f"Error: The folder '{folder_path}' does not exist.")
    
    return pdf_files

def create_qa_system(past_paper_paths, subject_book_paths):
    all_texts = ""
    for pdf_path in past_paper_paths + subject_book_paths:
        pdf_text = extract_text_from_pdf(pdf_path)
        all_texts += pdf_text + "\n"

    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
    texts = text_splitter.split_text(all_texts)

    embeddings = HuggingFaceEmbeddings()
    db = Chroma.from_texts(texts, embeddings)

    llm = ChatGroq(model="llama-3.1-70b-versatile", temperature=0.7)
    qa = RetrievalQA.from_chain_type(
        llm=llm, chain_type="stuff", retriever=db.as_retriever(search_kwargs={"k": 3})
    )

    return qa

def generate_question_paper(qa_system, num_questions):
    questions = []
    for _ in range(num_questions):
        prompt = (
            "Generate a new multiple choice exam which has 10 questions based "
            "on the content of the past papers, ensuring it's within the scope "
            "of the subject books. The question should be challenging but fair. "
            "Format the question in a structured manner suitable for an exam paper, "
            "including clear instructions. Give MCQs in the form of a, b, c, d options. "
            "Also, space between each question should be at least 2 lines in proper format."
        )
        response = qa_system.run(prompt)
        questions.append(response)
    return questions

def create_reformatted_question_paper_pdf(questions, output_path):
    doc = SimpleDocTemplate(output_path, pagesize=letter)
    styles = getSampleStyleSheet()
    content = []

    content.append(Paragraph("Generated Question Paper", styles['Title']))
    content.append(Spacer(1, 12))
    styles.add(ParagraphStyle(name='Justify', alignment=TA_JUSTIFY))

    for i, question in enumerate(questions, 1):
        content.append(Paragraph(f"Question {i}", styles['Heading2']))
        lines = question.split('\n')
        for line in lines:
            if line.strip():
                if line.startswith('*') or line.startswith('-'):
                    content.append(Paragraph(f"â€¢ {line.strip('*- ')}", styles['BodyText']))
                else:
                    content.append(Paragraph(line, styles['Justify']))
        content.append(Spacer(1, 12))

    doc.build(content)


def generate_related_mcq(llm, question, vector_store, k=1):
    """
    Generates a new MCQ based on the semantic search result.
    
    Parameters:
    - llm: The language model instance (e.g., ChatGroq).
    - question: The input question to search related content.
    - vector_store: The vector store for semantic search.
    - k: Number of relevant chunks to retrieve.

    Returns:
    - A new MCQ generated by the language model.
    """
    # Perform semantic search to get relevant chunks
    search_results = vector_store.similarity_search(question, k=k)
    
    # Extract the most relevant chunk's content (can concatenate if k > 1)
    response_chunk = "\n".join([doc.page_content for doc in search_results])
    
    print('=' * 10)  # For debugging
    print(response_chunk)  # Print the retrieved chunk
    print('=' * 10)

    # Formulate the prompt to generate a new MCQ
    prompt = f"""
    Based on the following information: {response_chunk}, generate a new multiple-choice question related to the topic in the following JSON format:
    
    {{
        "question": "Your new question here",
        "options": [
            "Option A",
            "Option B",
            "Option C",
            "Option D"
        ],
        "answer": "A"  # Correct option as one of A, B, C, or D
    }}
    Ensure the question and options are relevant to the topic provided. Other than the required format, dont write anything extra as I will be converting it into json parsing.
    """

    # Use the LLM to generate the new MCQ
    new_mcq_response = llm.invoke([HumanMessage(content=prompt)])

    return new_mcq_response.content

# Load all text files from the folder
def load_files_from_directory(directory):
    text_files = glob.glob(os.path.join(directory, "*.txt"))
    docs = []
    for file in text_files:
        try:
            with open(file, "r", encoding="utf-8") as f:
                text = f.read()
        except UnicodeDecodeError:
            with open(file, "r", encoding="ISO-8859-1") as f:  # Fallback to ISO-8859-1
                text = f.read()
        docs.append(text)
    return docs
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(BASE_DIR, "ocrbooks")
flag = False    # Means the things are not initialized
def generate_mcqs(subject_name, subject_questions):
    global flag
    if not flag:
        documents = load_files_from_directory(os.path.join(DATA_DIR, subject_name))
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,  # Adjust chunk size based on document size
            chunk_overlap=50,  # Overlap for context continuity
        )
        embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
        chunks = []
        for doc in documents:
            chunks.extend(text_splitter.split_text(doc))
        # Create ChromaDB vector store with the chunks
        persist_directory = os.path.join(BASE_DIR, f'embeddings/{subject_name}')
        # # Load or Create Chroma Vector Store
        if not os.path.exists(persist_directory):
            vector_store = Chroma.from_texts(chunks, embedding_model, persist_directory=persist_directory)
        else:
            vector_store = Chroma(persist_directory=persist_directory, embedding_function=embedding_model)
        llm = ChatGroq(model="llama-3.1-70b-versatile", temperature=0.7)
        # Create a RetrievalQA chain using a language model and vector store
        retrieval_chain = RetrievalQA.from_chain_type(
            llm= llm, 
            retriever=vector_store.as_retriever(),
            chain_type="stuff"
        )
        flag = True
    print("Things are generated", '='*10)
    new_mcqs = []
    # Iterate through the biology_questions DataFrame
    for index, row in subject_questions.iterrows():
        question = row["Question"]
        print(f"Generating Q{index+1}","="*10)
        print("Question:", question)
        new_mcq = generate_related_mcq(llm, question, vector_store)
        new_mcqs.append(new_mcq)
    
    cleaned_mcqs = []
    for mcq in new_mcqs:
        # Remove code block indicators ```json and ``` if present
        if mcq.startswith('```json'):
            mcq = mcq.replace('```json\n', '').replace('```', '')
        
        # Parse the cleaned string into a dictionary
        try:
            mcq_dict = json.loads(mcq)
            cleaned_mcqs.append(mcq_dict)
        except json.JSONDecodeError as e:
            print(f"Error decoding JSON: {e}")
            # Handle or skip invalid JSON
            continue

    # Assuming you want to store new MCQs in a DataFrame
    # new_mcqs_df = pd.DataFrame(new_mcqs, columns=["New MCQ"])
    print("Returning new MCQs DataFrame", '='*10)
    print("New MCQs DataFrame:", new_mcqs)
    return new_mcqs