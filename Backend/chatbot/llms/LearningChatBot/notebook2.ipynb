{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load .env file\n",
    "load_dotenv()\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "\n",
    "if not groq_api_key:\n",
    "    raise ValueError(\"Groq API key not found in .env file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\miniconda3\\envs\\fyp\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:132: UserWarning: Field \"model_name\" in HuggingFaceInferenceAPIEmbeddings has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\abdul\\AppData\\Local\\Temp\\ipykernel_18532\\3589759027.py:21: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "c:\\Users\\abdul\\miniconda3\\envs\\fyp\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "c:\\Users\\abdul\\miniconda3\\envs\\fyp\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import re\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.schema import Document\n",
    "\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the HuggingFace Embedding Model\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load documents from a text file and add metadata\n",
    "def load_documents(file_path):\n",
    "    loader = TextLoader(file_path)\n",
    "    documents = loader.load()\n",
    "    \n",
    "    # Add metadata to each document (e.g., file name)\n",
    "    for doc in documents:\n",
    "        doc.metadata[\"source\"] = file_path\n",
    "    return documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your subtopic and chapter separators\n",
    "\n",
    "# Custom RecursiveCharacterTextSplitter with regex patterns for subtopics and chapters\n",
    "class CustomTextSplitter(RecursiveCharacterTextSplitter):\n",
    "    def __init__(self, **kwargs):\n",
    "        subtopic_pattern = re.compile(r'(\\d+(\\.\\d+)+)')\n",
    "        chapter_separator = 'chapter end -------------------------------------'\n",
    "\n",
    "        # Initialize with any other parameters, and add your separators\n",
    "        super().__init__(separators=[chapter_separator], **kwargs)\n",
    "        self.subtopic_pattern = subtopic_pattern\n",
    "\n",
    "    def split_text(self, text):\n",
    "        # First, split by chapters\n",
    "        texts = super().split_text(text)\n",
    "        documents = []\n",
    "        \n",
    "        # For each chapter, split by subtopic using the subtopic regex\n",
    "        chapter_number = 1\n",
    "        for chapter in texts:\n",
    "            subtopic_splits = self._split_by_subtopic(chapter, chapter_number)\n",
    "            documents.extend(subtopic_splits)\n",
    "            chapter_number += 1\n",
    "        \n",
    "        return documents\n",
    "\n",
    "    def _split_by_subtopic(self, text, chapter_number):\n",
    "        # Use the subtopic regex to split text\n",
    "        matches = list(self.subtopic_pattern.finditer(text))\n",
    "        if not matches:\n",
    "            # No subtopics found, return the full text as a single Document\n",
    "            return [Document(page_content=text.strip(), metadata={\"chapter\": chapter_number})]\n",
    "        \n",
    "        subtopics = []\n",
    "        start_idx = 0\n",
    "        subtopic_number = 1\n",
    "        \n",
    "        for match in matches:\n",
    "            end_idx = match.start()\n",
    "            if start_idx != end_idx:\n",
    "                subtopics.append(Document(\n",
    "                    page_content=text[start_idx:end_idx].strip(),\n",
    "                    metadata={\"chapter\": chapter_number, \"subtopic\": subtopic_number}\n",
    "                ))\n",
    "            start_idx = end_idx\n",
    "            subtopic_number += 1\n",
    "            \n",
    "        # Append the remaining part as a subtopic\n",
    "        subtopics.append(Document(\n",
    "            page_content=text[start_idx:].strip(),\n",
    "            metadata={\"chapter\": chapter_number, \"subtopic\": subtopic_number}\n",
    "        ))\n",
    "        \n",
    "        return subtopics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings and handle storage\n",
    "def embed_documents(split_docs, embedding_model):\n",
    "    EMBEDDINGS_FOLDER = \"embeddings\"\n",
    "    EMBEDDINGS_FILE = os.path.join(EMBEDDINGS_FOLDER, \"emb01.pkl\")\n",
    "\n",
    "    if not os.path.exists(EMBEDDINGS_FOLDER):\n",
    "        os.makedirs(EMBEDDINGS_FOLDER)\n",
    "\n",
    "    if os.path.exists(EMBEDDINGS_FILE):\n",
    "        print(f\"Loading existing embeddings from {EMBEDDINGS_FILE}...\")\n",
    "        with open(EMBEDDINGS_FILE, 'rb') as f:\n",
    "            embedded_docs = pickle.load(f)\n",
    "    else:\n",
    "        print(\"Creating new embeddings...\")\n",
    "        texts = [doc.page_content for doc in split_docs]\n",
    "        embedded_docs = embedding_model.embed_documents(texts)\n",
    "\n",
    "        with open(EMBEDDINGS_FILE, 'wb') as f:\n",
    "            pickle.dump(embedded_docs, f)\n",
    "\n",
    "    return embedded_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store embeddings in Chroma vector store\n",
    "def store_embeddings(split_docs, embedding_model):\n",
    "    vector_store = Chroma.from_documents(split_docs, embedding_model) \n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getting_retriever(llm,vector_store):\n",
    "    # Opiton 01:\n",
    "    # Creating ContextualCompressionRetriever\n",
    "    compressor = LLMChainExtractor.from_llm(llm)\n",
    "    compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=vector_store.as_retriever(search_type = \"mmr\")\n",
    "    )\n",
    "    return compression_retriever\n",
    "    # Contextual Compression will find the relevant records and only contains the relevant data from chunks instead of whole chunks\n",
    "    # Maximum Marginal Relevance (mmr) is used to get diverse set of documents\n",
    "\n",
    "    # # Option 02:\n",
    "    # document_content_description = \"Content from text book\"\n",
    "    # metadata_field_info = [\n",
    "    #     AttributeInfo(\n",
    "    #         name=\"source\",\n",
    "    #         description=\"The chapter number from which the topic is taken from\",\n",
    "    #         type=\"string\",\n",
    "    #     )\n",
    "    # ]\n",
    "    # retriever = SelfQueryRetriever.from_llm(\n",
    "    #     llm,\n",
    "    #     vector_store,\n",
    "    #     document_content_description,\n",
    "    #     metadata_field_info,\n",
    "    #     # verbose=True\n",
    "    # )\n",
    "    # return retriever\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the LLM\n",
    "def initialize_llm():\n",
    "    llm = ChatGroq(\n",
    "        model=\"llama-3.1-70b-versatile\",\n",
    "        temperature=0.1,\n",
    "    )\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Querying the retriever and LLM\n",
    "def query_llm(llm, retriever, query, qa):\n",
    "    # Retrieve relevant documents\n",
    "    results = retriever.get_relevant_documents(query)\n",
    "\n",
    "    # Capture the actual text chunks used\n",
    "    relevant_texts = [doc.page_content for doc in results]\n",
    "\n",
    "    # Use the LLM to process the retrieved documents\n",
    "    if results:\n",
    "        # Combine results for the LLM prompt, and track their sources\n",
    "        context = \"\\n\".join(relevant_texts)\n",
    "        prompt = f\"\"\"Use relevant information from 9th to 12th-grade textbooks to answer the student's query. If the context is helpful, incorporate it; otherwise, provide a general explanation. Avoid mentioning irrelevance and instead say, \"I cannot find relevant data from your book but I will explain the general concept.\" Encourage the student to ask follow-up questions related to the topics in the books.\n",
    "        \n",
    "        Context:\n",
    "        {context}\n",
    "        Student Query:\n",
    "        {query}\n",
    "        \"\"\"\n",
    "        \n",
    "        response = qa({\"question\": prompt})\n",
    "        \n",
    "        # Return both the response and relevant texts\n",
    "        return response, relevant_texts\n",
    "    else:\n",
    "        return \"No relevant documents found.\", []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing embeddings from embeddings\\emb01.pkl...\n"
     ]
    }
   ],
   "source": [
    "# Main execution flow\n",
    "if __name__ == \"__main__\":\n",
    "    # Load your document\n",
    "    # documents = load_documents('resources/9thComputerScience_cleaned.txt')\n",
    "    with open('resources/9thComputerScience_cleaned.txt', 'r') as file:\n",
    "        documents = file.read()\n",
    "    # Split the text into smaller chunks\n",
    "    text_splitter = CustomTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "    split_docs = text_splitter.split_text(documents)\n",
    "    # print(split_docs[10])\n",
    "    embedded_docs = embed_documents(split_docs, embedding_model)\n",
    "    vector_store = store_embeddings(split_docs, embedding_model)\n",
    "    llm = initialize_llm()\n",
    "    retriever = getting_retriever(llm,vector_store)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "system_prompt = (\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If foudn relevant to user query then do use it otherwise give it  \"\n",
    "    \"from your own knowledge.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = rag_chain.invoke({\"input\": \"can you give me an interesting topic from 1st chapter\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'can you give me an interesting topic from 1st chapter',\n",
       " 'context': [Document(metadata={'chapter': 1, 'subtopic': 6}, page_content='1.1.3 Planning a Solution\\nAfter analyzing a problem, we formulate a plan that may lead us towards the solution of a problem. This phase includes finding the right strategy for problem solving. Some of the strategies are:\\ne Divide and Conquer: This strategy divides a complex problem into\\nsmaller problems. Figure 1-3 Planning for success\\ne Guess, Check and Improve: The designer guesses a solution to a problem and then checks the correctness of the solution. If the solution is not according to expectations, then he/she refines the solution. The refinement is an iterative process.\\ne Act it Out: In this strategy the designer defines the list of “to-do” tasks. Afterwards he/she performs the task.\\ne Prototype (Draw): This technique draws a pictorial representation of the solution. It is not the final solution. However, it may help a designer to understand the important components of the solution.')],\n",
       " 'answer': 'From the 1st chapter, an interesting topic could be \"Divide and Conquer\" strategy for problem-solving. This strategy involves breaking down a complex problem into smaller, more manageable sub-problems, making it easier to understand and solve. It\\'s a fascinating approach to tackle complex problems and can be applied to various fields, including computer science, mathematics, and even real-life situations.'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Keep the answer as concise as possible. Also ask the follow up questions for related topics or another topic from the same chapter. \n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\", \"question\"],template=template,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abdul\\AppData\\Local\\Temp\\ipykernel_18532\\603927876.py:17: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = qa({\"question\": query})\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Got multiple output keys: dict_keys(['answer', 'source_documents']), cannot determine which to store in memory. Please set the 'output_key' explicitly.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 17\u001b[0m\n\u001b[0;32m      7\u001b[0m qa \u001b[38;5;241m=\u001b[39m ConversationalRetrievalChain\u001b[38;5;241m.\u001b[39mfrom_llm(\n\u001b[0;32m      8\u001b[0m             llm,\n\u001b[0;32m      9\u001b[0m             retriever\u001b[38;5;241m=\u001b[39mretriever,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[38;5;66;03m# chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\u001b[39;00m\n\u001b[0;32m     15\u001b[0m         )\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# response, relevant_texts = query_llm(llm, retriever, query, qa)\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mqa\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Output response and relevant text chunks\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "File \u001b[1;32mc:\\Users\\abdul\\miniconda3\\envs\\fyp\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:179\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    177\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    178\u001b[0m     emit_warning()\n\u001b[1;32m--> 179\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\abdul\\miniconda3\\envs\\fyp\\Lib\\site-packages\\langchain\\chains\\base.py:389\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[0;32m    358\u001b[0m \n\u001b[0;32m    359\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    382\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[0;32m    387\u001b[0m }\n\u001b[1;32m--> 389\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    394\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\abdul\\miniconda3\\envs\\fyp\\Lib\\site-packages\\langchain\\chains\\base.py:170\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    169\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    171\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[1;32mc:\\Users\\abdul\\miniconda3\\envs\\fyp\\Lib\\site-packages\\langchain\\chains\\base.py:165\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[0;32m    159\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[0;32m    161\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    162\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    163\u001b[0m     )\n\u001b[1;32m--> 165\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprep_outputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    169\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32mc:\\Users\\abdul\\miniconda3\\envs\\fyp\\Lib\\site-packages\\langchain\\chains\\base.py:466\u001b[0m, in \u001b[0;36mChain.prep_outputs\u001b[1;34m(self, inputs, outputs, return_only_outputs)\u001b[0m\n\u001b[0;32m    464\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_outputs(outputs)\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 466\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_only_outputs:\n\u001b[0;32m    468\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mc:\\Users\\abdul\\miniconda3\\envs\\fyp\\Lib\\site-packages\\langchain\\memory\\chat_memory.py:55\u001b[0m, in \u001b[0;36mBaseChatMemory.save_context\u001b[1;34m(self, inputs, outputs)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_context\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any], outputs: Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Save context from this conversation to buffer.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m     input_str, output_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_input_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_memory\u001b[38;5;241m.\u001b[39madd_messages(\n\u001b[0;32m     57\u001b[0m         [HumanMessage(content\u001b[38;5;241m=\u001b[39minput_str), AIMessage(content\u001b[38;5;241m=\u001b[39moutput_str)]\n\u001b[0;32m     58\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\abdul\\miniconda3\\envs\\fyp\\Lib\\site-packages\\langchain\\memory\\chat_memory.py:44\u001b[0m, in \u001b[0;36mBaseChatMemory._get_input_output\u001b[1;34m(self, inputs, outputs)\u001b[0m\n\u001b[0;32m     38\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m     39\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m got multiple output keys:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     40\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutputs\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. The default \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m key is being used.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     41\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m If this is not desired, please manually set \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_key\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     42\u001b[0m         )\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 44\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     45\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot multiple output keys: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutputs\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, cannot \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     46\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetermine which to store in memory. Please set the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     47\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_key\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m explicitly.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     48\u001b[0m         )\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     50\u001b[0m     output_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key\n",
      "\u001b[1;31mValueError\u001b[0m: Got multiple output keys: dict_keys(['answer', 'source_documents']), cannot determine which to store in memory. Please set the 'output_key' explicitly."
     ]
    }
   ],
   "source": [
    "# Example query \n",
    "query = \"can you tell me an interesting topic from chapter 1\"\n",
    "memory = ConversationBufferMemory(\n",
    "            memory_key=\"chat_history\",\n",
    "            return_messages=True\n",
    "        )\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "            llm,\n",
    "            retriever=retriever,\n",
    "            memory=memory,\n",
    "            return_source_documents=True,\n",
    "            # metadata={\"output_key\": \"answer\"},\n",
    "            # output_messages_key = 'answer'\n",
    "            # chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    "        )\n",
    "# response, relevant_texts = query_llm(llm, retriever, query, qa)\n",
    "result = qa({\"question\": query})\n",
    "\n",
    "# Output response and relevant text chunks\n",
    "print(result)\n",
    "# print(\"==============================================\")\n",
    "# print(\"\\nRelevant text chunks used in the response:\")\n",
    "# for text in relevant_texts:\n",
    "#     print(\"Chunk: ==============================\")\n",
    "#     print(text[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question can you tell me an interesting topic from chapter 1\n",
      "chat_history [HumanMessage(content='can you tell me an interesting topic from chapter 1', additional_kwargs={}, response_metadata={}), AIMessage(content='One interesting topic from chapter 1 is the different strategies for problem-solving, specifically the \"Divide and Conquer\" strategy, which involves breaking down a complex problem into smaller, more manageable problems.', additional_kwargs={}, response_metadata={})]\n",
      "answer One interesting topic from chapter 1 is the different strategies for problem-solving, specifically the \"Divide and Conquer\" strategy, which involves breaking down a complex problem into smaller, more manageable problems.\n"
     ]
    }
   ],
   "source": [
    "for key, val in result.items():\n",
    "    print(key, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question : Yes tell me how to apply them\n",
      "chat_history : [HumanMessage(content='can you tell me an interesting topic from chapter 1', additional_kwargs={}, response_metadata={}), AIMessage(content='One interesting topic from chapter 1 is the different strategies for problem-solving, specifically the \"Divide and Conquer\" strategy, which involves breaking down a complex problem into smaller, more manageable problems.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Yes tell me how to apply them', additional_kwargs={}, response_metadata={}), AIMessage(content='The \"Divide and Conquer\" strategy is a problem-solving approach that involves breaking down a complex problem into smaller, more manageable sub-problems. Here\\'s a step-by-step guide on how to apply this strategy:\\n\\n1. **Identify the problem**: Clearly define the problem you want to solve. Understand the problem\\'s scope, constraints, and goals.\\n2. **Break down the problem**: Divide the problem into smaller sub-problems that are more manageable and easier to solve. These sub-problems should be independent of each other, but still related to the original problem.\\n3. **Solve each sub-problem**: Focus on solving each sub-problem individually. Use the same problem-solving techniques, such as analysis, reasoning, and creativity, to find solutions to each sub-problem.\\n4. **Combine the solutions**: Once you have solved each sub-problem, combine the solutions to form a complete solution to the original problem.\\n5. **Verify the solution**: Test and verify the combined solution to ensure it meets the original problem\\'s requirements and constraints.\\n\\nSome key considerations when applying the \"Divide and Conquer\" strategy:\\n\\n* **Ensure sub-problems are independent**: Each sub-problem should be able to be solved independently without affecting the other sub-problems.\\n* **Choose the right sub-problems**: Select sub-problems that are meaningful and relevant to the original problem.\\n* **Use a hierarchical approach**: Break down complex problems into smaller sub-problems, and then break down those sub-problems into even smaller ones, if necessary.\\n* **Combine solutions carefully**: When combining solutions, ensure that the individual solutions work together seamlessly to form a complete solution.\\n\\nBy following these steps and considerations, you can effectively apply the \"Divide and Conquer\" strategy to solve complex problems.', additional_kwargs={}, response_metadata={})]\n",
      "answer : The \"Divide and Conquer\" strategy is a problem-solving approach that involves breaking down a complex problem into smaller, more manageable sub-problems. Here's a step-by-step guide on how to apply this strategy:\n",
      "\n",
      "1. **Identify the problem**: Clearly define the problem you want to solve. Understand the problem's scope, constraints, and goals.\n",
      "2. **Break down the problem**: Divide the problem into smaller sub-problems that are more manageable and easier to solve. These sub-problems should be independent of each other, but still related to the original problem.\n",
      "3. **Solve each sub-problem**: Focus on solving each sub-problem individually. Use the same problem-solving techniques, such as analysis, reasoning, and creativity, to find solutions to each sub-problem.\n",
      "4. **Combine the solutions**: Once you have solved each sub-problem, combine the solutions to form a complete solution to the original problem.\n",
      "5. **Verify the solution**: Test and verify the combined solution to ensure it meets the original problem's requirements and constraints.\n",
      "\n",
      "Some key considerations when applying the \"Divide and Conquer\" strategy:\n",
      "\n",
      "* **Ensure sub-problems are independent**: Each sub-problem should be able to be solved independently without affecting the other sub-problems.\n",
      "* **Choose the right sub-problems**: Select sub-problems that are meaningful and relevant to the original problem.\n",
      "* **Use a hierarchical approach**: Break down complex problems into smaller sub-problems, and then break down those sub-problems into even smaller ones, if necessary.\n",
      "* **Combine solutions carefully**: When combining solutions, ensure that the individual solutions work together seamlessly to form a complete solution.\n",
      "\n",
      "By following these steps and considerations, you can effectively apply the \"Divide and Conquer\" strategy to solve complex problems.\n"
     ]
    }
   ],
   "source": [
    "result = qa({\"question\": \"Yes tell me how to apply them\"})\n",
    "for key, val in result.items():\n",
    "    print(key,':', val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['question', 'chat_history', 'answer'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================\n",
      "1.1.3 Planning a Solution\n",
      "After analyzing a problem, we formulate a plan that may lead us towards the solution of a problem. This phase includes finding the right strategy for problem solving. Some of the strategies are:\n",
      "e Divide and Conquer: This strategy divides a complex problem into\n",
      "smaller problems. Figure 1-3 Planning for success\n",
      "e Guess, Check and Improve: The designer guesses a solution to a problem and then checks the correctness of the solution. If the solution is not according to expectations, then he/she refines the solution. The refinement is an iterative process.\n",
      "e Act it Out: In this strategy the designer defines the list of “to-do” tasks. Afterwards he/she performs the task.\n",
      "e Prototype (Draw): This technique draws a pictorial representation of the solution. It is not the final solution. However, it may help a designer to understand the important components of the solution.\n"
     ]
    }
   ],
   "source": [
    "for text in relevant_texts:\n",
    "    print(\"=====================================\")\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m response, relevant_texts \u001b[38;5;241m=\u001b[39m query_llm(llm, retriever, query, qa)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Output response and relevant text chunks\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mresponse\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43manswer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m==============================================\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mRelevant text chunks used in the response:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: string indices must be integers, not 'str'"
     ]
    }
   ],
   "source": [
    "query = \"yes tell me how to apply them\"\n",
    "response, relevant_texts = query_llm(llm, retriever, query, qa)\n",
    "\n",
    "# Output response and relevant text chunks\n",
    "print(response['answer'])\n",
    "print(\"==============================================\")\n",
    "print(\"\\nRelevant text chunks used in the response:\")\n",
    "for text in relevant_texts:\n",
    "    print(\"Chunk: ==============================\")\n",
    "    print(text[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No relevant documents found.'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example query \n",
    "# query = \"who gave the idea of boolean values and on what date\"\n",
    "# response, relevant_texts = query_llm(llm, retriever, query)\n",
    "\n",
    "# # Output response and relevant text chunks\n",
    "# print(response.content)\n",
    "# print(\"==============================================\")\n",
    "# print(\"\\nRelevant text chunks used in the response:\")\n",
    "# for text in relevant_texts:\n",
    "#     print(\"Chunk: ==============================\")\n",
    "#     print(text[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example query \n",
    "# query = \"how ip4 and ip6 works\"\n",
    "# response, relevant_texts = query_llm(llm, retriever, query)\n",
    "\n",
    "# # Output response and relevant text chunks\n",
    "# print(response.content)\n",
    "# print(\"==============================================\")\n",
    "# print(\"\\nRelevant text chunks used in the response:\")\n",
    "# for text in relevant_texts:\n",
    "#     print(\"Chunk: ==============================\")\n",
    "#     print(text[:300])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
