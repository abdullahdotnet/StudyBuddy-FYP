{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load .env file\n",
    "load_dotenv()\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "\n",
    "if not groq_api_key:\n",
    "    raise ValueError(\"Groq API key not found in .env file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zubay\\.conda\\envs\\fyp\\Lib\\importlib\\__init__.py:90: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  return _bootstrap._gcd_import(name[level:], package, level)\n",
      "C:\\Users\\zubay\\AppData\\Local\\Temp\\ipykernel_19864\\3589759027.py:21: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "c:\\Users\\zubay\\.conda\\envs\\fyp\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "c:\\Users\\zubay\\.conda\\envs\\fyp\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import re\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.schema import Document\n",
    "\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the HuggingFace Embedding Model\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load documents from a text file and add metadata\n",
    "def load_documents(file_path):\n",
    "    loader = TextLoader(file_path)\n",
    "    documents = loader.load()\n",
    "    \n",
    "    # Add metadata to each document (e.g., file name)\n",
    "    for doc in documents:\n",
    "        doc.metadata[\"source\"] = file_path\n",
    "    return documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your subtopic and chapter separators\n",
    "\n",
    "# Custom RecursiveCharacterTextSplitter with regex patterns for subtopics and chapters\n",
    "class CustomTextSplitter(RecursiveCharacterTextSplitter):\n",
    "    def __init__(self, **kwargs):\n",
    "        subtopic_pattern = re.compile(r'(\\d+(\\.\\d+)+)')\n",
    "        chapter_separator = 'chapter end -------------------------------------'\n",
    "\n",
    "        # Initialize with any other parameters, and add your separators\n",
    "        super().__init__(separators=[chapter_separator], **kwargs)\n",
    "        self.subtopic_pattern = subtopic_pattern\n",
    "\n",
    "    def split_text(self, text):\n",
    "        # First, split by chapters\n",
    "        texts = super().split_text(text)\n",
    "        documents = []\n",
    "        \n",
    "        # For each chapter, split by subtopic using the subtopic regex\n",
    "        chapter_number = 1\n",
    "        for chapter in texts:\n",
    "            subtopic_splits = self._split_by_subtopic(chapter, chapter_number)\n",
    "            documents.extend(subtopic_splits)\n",
    "            chapter_number += 1\n",
    "        \n",
    "        return documents\n",
    "\n",
    "    def _split_by_subtopic(self, text, chapter_number):\n",
    "        # Use the subtopic regex to split text\n",
    "        matches = list(self.subtopic_pattern.finditer(text))\n",
    "        if not matches:\n",
    "            # No subtopics found, return the full text as a single Document\n",
    "            return [Document(page_content=text.strip(), metadata={\"chapter\": chapter_number})]\n",
    "        \n",
    "        subtopics = []\n",
    "        start_idx = 0\n",
    "        subtopic_number = 1\n",
    "        \n",
    "        for match in matches:\n",
    "            end_idx = match.start()\n",
    "            if start_idx != end_idx:\n",
    "                subtopics.append(Document(\n",
    "                    page_content=text[start_idx:end_idx].strip(),\n",
    "                    metadata={\"chapter\": chapter_number, \"subtopic\": subtopic_number}\n",
    "                ))\n",
    "            start_idx = end_idx\n",
    "            subtopic_number += 1\n",
    "            \n",
    "        # Append the remaining part as a subtopic\n",
    "        subtopics.append(Document(\n",
    "            page_content=text[start_idx:].strip(),\n",
    "            metadata={\"chapter\": chapter_number, \"subtopic\": subtopic_number}\n",
    "        ))\n",
    "        \n",
    "        return subtopics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings and handle storage\n",
    "def embed_documents(split_docs, embedding_model):\n",
    "    EMBEDDINGS_FOLDER = \"embeddings\"\n",
    "    EMBEDDINGS_FILE = os.path.join(EMBEDDINGS_FOLDER, \"emb01.pkl\")\n",
    "\n",
    "    if not os.path.exists(EMBEDDINGS_FOLDER):\n",
    "        os.makedirs(EMBEDDINGS_FOLDER)\n",
    "\n",
    "    if os.path.exists(EMBEDDINGS_FILE):\n",
    "        print(f\"Loading existing embeddings from {EMBEDDINGS_FILE}...\")\n",
    "        with open(EMBEDDINGS_FILE, 'rb') as f:\n",
    "            embedded_docs = pickle.load(f)\n",
    "    else:\n",
    "        print(\"Creating new embeddings...\")\n",
    "        texts = [doc.page_content for doc in split_docs]\n",
    "        embedded_docs = embedding_model.embed_documents(texts)\n",
    "\n",
    "        with open(EMBEDDINGS_FILE, 'wb') as f:\n",
    "            pickle.dump(embedded_docs, f)\n",
    "\n",
    "    return embedded_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store embeddings in Chroma vector store\n",
    "def store_embeddings(split_docs, embedding_model):\n",
    "    vector_store = Chroma.from_documents(split_docs, embedding_model) \n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getting_retriever(llm,vector_store):\n",
    "    # Opiton 01:\n",
    "    # Creating ContextualCompressionRetriever\n",
    "    compressor = LLMChainExtractor.from_llm(llm)\n",
    "    compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=vector_store.as_retriever(search_type = \"mmr\")\n",
    "    )\n",
    "    return compression_retriever\n",
    "    # Contextual Compression will find the relevant records and only contains the relevant data from chunks instead of whole chunks\n",
    "    # Maximum Marginal Relevance (mmr) is used to get diverse set of documents\n",
    "\n",
    "    # # Option 02:\n",
    "    # document_content_description = \"Content from text book\"\n",
    "    # metadata_field_info = [\n",
    "    #     AttributeInfo(\n",
    "    #         name=\"source\",\n",
    "    #         description=\"The chapter number from which the topic is taken from\",\n",
    "    #         type=\"string\",\n",
    "    #     )\n",
    "    # ]\n",
    "    # retriever = SelfQueryRetriever.from_llm(\n",
    "    #     llm,\n",
    "    #     vector_store,\n",
    "    #     document_content_description,\n",
    "    #     metadata_field_info,\n",
    "    #     # verbose=True\n",
    "    # )\n",
    "    # return retriever\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the LLM\n",
    "def initialize_llm():\n",
    "    llm = ChatGroq(\n",
    "        model=\"llama-3.1-70b-versatile\",\n",
    "        temperature=0.1,\n",
    "    )\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Querying the retriever and LLM\n",
    "def query_llm(llm, retriever, query, qa):\n",
    "    # Retrieve relevant documents\n",
    "    results = retriever.get_relevant_documents(query)\n",
    "\n",
    "    # Capture the actual text chunks used\n",
    "    relevant_texts = [doc.page_content for doc in results]\n",
    "\n",
    "    # Use the LLM to process the retrieved documents\n",
    "    if results:\n",
    "        # Combine results for the LLM prompt, and track their sources\n",
    "        context = \"\\n\".join(relevant_texts)\n",
    "        prompt = f\"\"\"Use relevant information from 9th to 12th-grade textbooks to answer the student's query. If the context is helpful, incorporate it; otherwise, provide a general explanation. Avoid mentioning irrelevance and instead say, \"I cannot find relevant data from your book but I will explain the general concept.\" Encourage the student to ask follow-up questions related to the topics in the books.\n",
    "        \n",
    "        Context:\n",
    "        {context}\n",
    "        Student Query:\n",
    "        {query}\n",
    "        \"\"\"\n",
    "        \n",
    "        response = qa({\"question\": prompt})\n",
    "        \n",
    "        # Return both the response and relevant texts\n",
    "        return response, relevant_texts\n",
    "    else:\n",
    "        return \"No relevant documents found.\", []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing embeddings from embeddings\\emb01.pkl...\n"
     ]
    }
   ],
   "source": [
    "# Main execution flow\n",
    "if __name__ == \"__main__\":\n",
    "    # Load your document\n",
    "    # documents = load_documents('resources/9thComputerScience_cleaned.txt')\n",
    "    with open('resources/9thComputerScience_cleaned.txt', 'r') as file:\n",
    "        documents = file.read()\n",
    "    # Split the text into smaller chunks\n",
    "    text_splitter = CustomTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "    split_docs = text_splitter.split_text(documents)\n",
    "    # print(split_docs[10])\n",
    "    embedded_docs = embed_documents(split_docs, embedding_model)\n",
    "    vector_store = store_embeddings(split_docs, embedding_model)\n",
    "    llm = initialize_llm()\n",
    "    retriever = getting_retriever(llm,vector_store)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "system_prompt = (\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If foudn relevant to user query then do use it otherwise give it  \"\n",
    "    \"from your own knowledge.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = rag_chain.invoke({\"input\": \"can you give me an interesting topic from 1st chapter\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'can you give me an interesting topic from 1st chapter',\n",
       " 'context': [Document(metadata={'chapter': 1, 'subtopic': 6}, page_content='1.1.3 Planning a Solution\\nAfter analyzing a problem, we formulate a plan that may lead us towards the solution of a problem. This phase includes finding the right strategy for problem solving. Some of the strategies are:\\ne Divide and Conquer: This strategy divides a complex problem into\\nsmaller problems. Figure 1-3 Planning for success\\ne Guess, Check and Improve: The designer guesses a solution to a problem and then checks the correctness of the solution. If the solution is not according to expectations, then he/she refines the solution. The refinement is an iterative process.\\ne Act it Out: In this strategy the designer defines the list of “to-do” tasks. Afterwards he/she performs the task.\\ne Prototype (Draw): This technique draws a pictorial representation of the solution. It is not the final solution. However, it may help a designer to understand the important components of the solution.')],\n",
       " 'answer': 'From the 1st chapter, an interesting topic could be \"Divide and Conquer\" strategy. This strategy involves breaking down a complex problem into smaller, more manageable problems, making it easier to solve. It\\'s a fascinating approach to problem-solving that can be applied to various real-life situations.'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Keep the answer as concise as possible. Also ask the follow up questions for related topics or another topic from the same chapter. \n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\", \"question\"],template=template,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for ConversationalRetrievalChain\nreturn_only_outputs\n  Extra inputs are not permitted [type=extra_forbidden, input_value=True, input_type=bool]\n    For further information visit https://errors.pydantic.dev/2.9/v/extra_forbidden",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 7\u001b[0m\n\u001b[0;32m      2\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcan you tell me an interesting topic from chapter 1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m memory \u001b[38;5;241m=\u001b[39m ConversationBufferMemory(\n\u001b[0;32m      4\u001b[0m             memory_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat_history\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m             return_messages\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m      6\u001b[0m         )\n\u001b[1;32m----> 7\u001b[0m qa \u001b[38;5;241m=\u001b[39m \u001b[43mConversationalRetrievalChain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_llm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m            \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m            \u001b[49m\u001b[43mretriever\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretriever\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_source_documents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# metadata={\"output_key\": \"answer\"},\u001b[39;49;00m\n\u001b[0;32m     14\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# output_messages_key = 'answer'\u001b[39;49;00m\n\u001b[0;32m     15\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\u001b[39;49;00m\n\u001b[0;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# response, relevant_texts = query_llm(llm, retriever, query, qa)\u001b[39;00m\n\u001b[0;32m     18\u001b[0m result \u001b[38;5;241m=\u001b[39m qa({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: query})\n",
      "File \u001b[1;32mc:\\Users\\zubay\\.conda\\envs\\fyp\\Lib\\site-packages\\langchain\\chains\\conversational_retrieval\\base.py:466\u001b[0m, in \u001b[0;36mConversationalRetrievalChain.from_llm\u001b[1;34m(cls, llm, retriever, condense_question_prompt, chain_type, verbose, condense_question_llm, combine_docs_chain_kwargs, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    459\u001b[0m _llm \u001b[38;5;241m=\u001b[39m condense_question_llm \u001b[38;5;129;01mor\u001b[39;00m llm\n\u001b[0;32m    460\u001b[0m condense_question_chain \u001b[38;5;241m=\u001b[39m LLMChain(\n\u001b[0;32m    461\u001b[0m     llm\u001b[38;5;241m=\u001b[39m_llm,\n\u001b[0;32m    462\u001b[0m     prompt\u001b[38;5;241m=\u001b[39mcondense_question_prompt,\n\u001b[0;32m    463\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m    464\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m    465\u001b[0m )\n\u001b[1;32m--> 466\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretriever\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretriever\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    468\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcombine_docs_chain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoc_chain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    469\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquestion_generator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcondense_question_chain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    470\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    471\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    472\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\zubay\\.conda\\envs\\fyp\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:214\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    212\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    213\u001b[0m     emit_warning()\n\u001b[1;32m--> 214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\zubay\\.conda\\envs\\fyp\\Lib\\site-packages\\langchain_core\\load\\serializable.py:110\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    109\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[1;32m--> 110\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\zubay\\.conda\\envs\\fyp\\Lib\\site-packages\\pydantic\\main.py:212\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(self, **data)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[0;32m    211\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 212\u001b[0m validated_self \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[0;32m    214\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    215\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    216\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    217\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    218\u001b[0m         category\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    219\u001b[0m     )\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for ConversationalRetrievalChain\nreturn_only_outputs\n  Extra inputs are not permitted [type=extra_forbidden, input_value=True, input_type=bool]\n    For further information visit https://errors.pydantic.dev/2.9/v/extra_forbidden"
     ]
    }
   ],
   "source": [
    "# Example query \n",
    "query = \"can you tell me an interesting topic from chapter 1\"\n",
    "memory = ConversationBufferMemory(\n",
    "            memory_key=\"chat_history\",\n",
    "            return_messages=True\n",
    "        )\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "            llm,\n",
    "            retriever=retriever,\n",
    "            memory=memory,\n",
    "            return_source_documents=True,\n",
    "            # metadata={\"output_key\": \"answer\"},\n",
    "            # output_messages_key = 'answer'\n",
    "            # chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    "        )\n",
    "# response, relevant_texts = query_llm(llm, retriever, query, qa)\n",
    "result = qa({\"question\": query})\n",
    "\n",
    "# Output response and relevant text chunks\n",
    "print(result)\n",
    "# print(\"==============================================\")\n",
    "# print(\"\\nRelevant text chunks used in the response:\")\n",
    "# for text in relevant_texts:\n",
    "#     print(\"Chunk: ==============================\")\n",
    "#     print(text[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question can you tell me an interesting topic from chapter 1\n",
      "chat_history [HumanMessage(content='can you tell me an interesting topic from chapter 1', additional_kwargs={}, response_metadata={}), AIMessage(content='One interesting topic from chapter 1 is the different strategies for problem-solving, specifically the \"Divide and Conquer\" strategy, which involves breaking down a complex problem into smaller, more manageable problems.', additional_kwargs={}, response_metadata={})]\n",
      "answer One interesting topic from chapter 1 is the different strategies for problem-solving, specifically the \"Divide and Conquer\" strategy, which involves breaking down a complex problem into smaller, more manageable problems.\n"
     ]
    }
   ],
   "source": [
    "for key, val in result.items():\n",
    "    print(key, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question : Yes tell me how to apply them\n",
      "chat_history : [HumanMessage(content='can you tell me an interesting topic from chapter 1', additional_kwargs={}, response_metadata={}), AIMessage(content='One interesting topic from chapter 1 is the different strategies for problem-solving, specifically the \"Divide and Conquer\" strategy, which involves breaking down a complex problem into smaller, more manageable problems.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Yes tell me how to apply them', additional_kwargs={}, response_metadata={}), AIMessage(content='The \"Divide and Conquer\" strategy is a problem-solving approach that involves breaking down a complex problem into smaller, more manageable sub-problems. Here\\'s a step-by-step guide on how to apply this strategy:\\n\\n1. **Identify the problem**: Clearly define the problem you want to solve. Understand the problem\\'s scope, constraints, and goals.\\n2. **Break down the problem**: Divide the problem into smaller sub-problems that are more manageable and easier to solve. These sub-problems should be independent of each other, but still related to the original problem.\\n3. **Solve each sub-problem**: Focus on solving each sub-problem individually. Use the same problem-solving techniques, such as analysis, reasoning, and creativity, to find solutions to each sub-problem.\\n4. **Combine the solutions**: Once you have solved each sub-problem, combine the solutions to form a complete solution to the original problem.\\n5. **Verify the solution**: Test and verify the combined solution to ensure it meets the original problem\\'s requirements and constraints.\\n\\nSome key considerations when applying the \"Divide and Conquer\" strategy:\\n\\n* **Ensure sub-problems are independent**: Each sub-problem should be able to be solved independently without affecting the other sub-problems.\\n* **Choose the right sub-problems**: Select sub-problems that are meaningful and relevant to the original problem.\\n* **Use a hierarchical approach**: Break down complex problems into smaller sub-problems, and then break down those sub-problems into even smaller ones, if necessary.\\n* **Combine solutions carefully**: When combining solutions, ensure that the individual solutions work together seamlessly to form a complete solution.\\n\\nBy following these steps and considerations, you can effectively apply the \"Divide and Conquer\" strategy to solve complex problems.', additional_kwargs={}, response_metadata={})]\n",
      "answer : The \"Divide and Conquer\" strategy is a problem-solving approach that involves breaking down a complex problem into smaller, more manageable sub-problems. Here's a step-by-step guide on how to apply this strategy:\n",
      "\n",
      "1. **Identify the problem**: Clearly define the problem you want to solve. Understand the problem's scope, constraints, and goals.\n",
      "2. **Break down the problem**: Divide the problem into smaller sub-problems that are more manageable and easier to solve. These sub-problems should be independent of each other, but still related to the original problem.\n",
      "3. **Solve each sub-problem**: Focus on solving each sub-problem individually. Use the same problem-solving techniques, such as analysis, reasoning, and creativity, to find solutions to each sub-problem.\n",
      "4. **Combine the solutions**: Once you have solved each sub-problem, combine the solutions to form a complete solution to the original problem.\n",
      "5. **Verify the solution**: Test and verify the combined solution to ensure it meets the original problem's requirements and constraints.\n",
      "\n",
      "Some key considerations when applying the \"Divide and Conquer\" strategy:\n",
      "\n",
      "* **Ensure sub-problems are independent**: Each sub-problem should be able to be solved independently without affecting the other sub-problems.\n",
      "* **Choose the right sub-problems**: Select sub-problems that are meaningful and relevant to the original problem.\n",
      "* **Use a hierarchical approach**: Break down complex problems into smaller sub-problems, and then break down those sub-problems into even smaller ones, if necessary.\n",
      "* **Combine solutions carefully**: When combining solutions, ensure that the individual solutions work together seamlessly to form a complete solution.\n",
      "\n",
      "By following these steps and considerations, you can effectively apply the \"Divide and Conquer\" strategy to solve complex problems.\n"
     ]
    }
   ],
   "source": [
    "result = qa({\"question\": \"Yes tell me how to apply them\"})\n",
    "for key, val in result.items():\n",
    "    print(key,':', val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['question', 'chat_history', 'answer'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================\n",
      "1.1.3 Planning a Solution\n",
      "After analyzing a problem, we formulate a plan that may lead us towards the solution of a problem. This phase includes finding the right strategy for problem solving. Some of the strategies are:\n",
      "e Divide and Conquer: This strategy divides a complex problem into\n",
      "smaller problems. Figure 1-3 Planning for success\n",
      "e Guess, Check and Improve: The designer guesses a solution to a problem and then checks the correctness of the solution. If the solution is not according to expectations, then he/she refines the solution. The refinement is an iterative process.\n",
      "e Act it Out: In this strategy the designer defines the list of “to-do” tasks. Afterwards he/she performs the task.\n",
      "e Prototype (Draw): This technique draws a pictorial representation of the solution. It is not the final solution. However, it may help a designer to understand the important components of the solution.\n"
     ]
    }
   ],
   "source": [
    "for text in relevant_texts:\n",
    "    print(\"=====================================\")\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m response, relevant_texts \u001b[38;5;241m=\u001b[39m query_llm(llm, retriever, query, qa)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Output response and relevant text chunks\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mresponse\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43manswer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m==============================================\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mRelevant text chunks used in the response:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: string indices must be integers, not 'str'"
     ]
    }
   ],
   "source": [
    "query = \"yes tell me how to apply them\"\n",
    "response, relevant_texts = query_llm(llm, retriever, query, qa)\n",
    "\n",
    "# Output response and relevant text chunks\n",
    "print(response['answer'])\n",
    "print(\"==============================================\")\n",
    "print(\"\\nRelevant text chunks used in the response:\")\n",
    "for text in relevant_texts:\n",
    "    print(\"Chunk: ==============================\")\n",
    "    print(text[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No relevant documents found.'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example query \n",
    "# query = \"who gave the idea of boolean values and on what date\"\n",
    "# response, relevant_texts = query_llm(llm, retriever, query)\n",
    "\n",
    "# # Output response and relevant text chunks\n",
    "# print(response.content)\n",
    "# print(\"==============================================\")\n",
    "# print(\"\\nRelevant text chunks used in the response:\")\n",
    "# for text in relevant_texts:\n",
    "#     print(\"Chunk: ==============================\")\n",
    "#     print(text[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example query \n",
    "# query = \"how ip4 and ip6 works\"\n",
    "# response, relevant_texts = query_llm(llm, retriever, query)\n",
    "\n",
    "# # Output response and relevant text chunks\n",
    "# print(response.content)\n",
    "# print(\"==============================================\")\n",
    "# print(\"\\nRelevant text chunks used in the response:\")\n",
    "# for text in relevant_texts:\n",
    "#     print(\"Chunk: ==============================\")\n",
    "#     print(text[:300])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
