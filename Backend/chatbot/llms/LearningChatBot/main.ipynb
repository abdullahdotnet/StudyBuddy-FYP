{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load .env file\n",
    "load_dotenv()\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "if not groq_api_key:\n",
    "    raise ValueError(\"Groq API key not found in .env file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for precessing text\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "# imports for creating pipeline for rag\n",
    "from langchain.embeddings import HuggingFaceEmbeddings      # for embeddings\n",
    "from langchain.vectorstores import Chroma                  # for vector store\n",
    "from langchain.document_loaders import TextLoader        # for loading text\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  # acting as base class for splitting text \n",
    "from langchain_groq import ChatGroq                         # for initializing LLM from groq\n",
    "from langchain.schema import Document                   # converting simple text to document object\n",
    "\n",
    "# For compressing the context of retrieved documents\n",
    "from langchain.retrievers import ContextualCompressionRetriever     # for compressing retrieved documents context\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor     # used in compression\n",
    "\n",
    "# For creating the retrieval chain for chat history\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# For creating history aware retriever\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_core.messages import AIMessage, HumanMessage     # for messages in history aware retriever\n",
    "\n",
    "\n",
    "# Extra for now\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zubay\\AppData\\Local\\Temp\\ipykernel_3516\\1594320778.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "c:\\Users\\zubay\\.conda\\envs\\fyp\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "c:\\Users\\zubay\\.conda\\envs\\fyp\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize the HuggingFace Embedding Model\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents(file_path):\n",
    "    loader = TextLoader('resources/9thComputerScience_cleaned.txt')\n",
    "    documents = loader.load()\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom RecursiveCharacterTextSplitter with regex patterns for subtopics and chapters\n",
    "class CustomTextSplitter(RecursiveCharacterTextSplitter):\n",
    "    def __init__(self, **kwargs):\n",
    "        subtopic_pattern = re.compile(r'(\\d+(\\.\\d+)+)')\n",
    "        chapter_separator = 'chapter end -------------------------------------'\n",
    "\n",
    "        # Initialize with any other parameters, and add your separators\n",
    "        super().__init__(separators=[chapter_separator], **kwargs)\n",
    "        self.subtopic_pattern = subtopic_pattern\n",
    "\n",
    "    def split_text(self, text):\n",
    "        # First, split by chapters\n",
    "        texts = super().split_text(text)\n",
    "        documents = []\n",
    "        \n",
    "        # For each chapter, split by subtopic using the subtopic regex\n",
    "        chapter_number = 1\n",
    "        for chapter in texts:\n",
    "            subtopic_splits = self._split_by_subtopic(chapter, chapter_number)\n",
    "            documents.extend(subtopic_splits)\n",
    "            chapter_number += 1\n",
    "        \n",
    "        return documents\n",
    "\n",
    "    def _split_by_subtopic(self, text, chapter_number):\n",
    "        # Use the subtopic regex to split text\n",
    "        matches = list(self.subtopic_pattern.finditer(text))\n",
    "        if not matches:\n",
    "            # No subtopics found, return the full text as a single Document\n",
    "            return [Document(page_content=text.strip(), metadata={\"chapter\": chapter_number})]\n",
    "        \n",
    "        subtopics = []\n",
    "        start_idx = 0\n",
    "        subtopic_number = 1\n",
    "        \n",
    "        for match in matches:\n",
    "            end_idx = match.start()\n",
    "            if start_idx != end_idx:\n",
    "                subtopics.append(Document(\n",
    "                    page_content=text[start_idx:end_idx].strip(),\n",
    "                    metadata={\"chapter\": chapter_number, \"subtopic\": subtopic_number}\n",
    "                ))\n",
    "            start_idx = end_idx\n",
    "            subtopic_number += 1\n",
    "            \n",
    "        # Append the remaining part as a subtopic\n",
    "        subtopics.append(Document(\n",
    "            page_content=text[start_idx:].strip(),\n",
    "            metadata={\"chapter\": chapter_number, \"subtopic\": subtopic_number}\n",
    "        ))\n",
    "        \n",
    "        return subtopics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings and handle storage\n",
    "def embed_documents(split_docs, embedding_model):\n",
    "    EMBEDDINGS_FOLDER = \"embeddings\"\n",
    "    EMBEDDINGS_FILE = os.path.join(EMBEDDINGS_FOLDER, \"emb01.pkl\")\n",
    "\n",
    "    if not os.path.exists(EMBEDDINGS_FOLDER):\n",
    "        os.makedirs(EMBEDDINGS_FOLDER)\n",
    "\n",
    "    if os.path.exists(EMBEDDINGS_FILE):\n",
    "        print(f\"Loading existing embeddings from {EMBEDDINGS_FILE}...\")\n",
    "        with open(EMBEDDINGS_FILE, 'rb') as f:\n",
    "            embedded_docs = pickle.load(f)\n",
    "            print(\"Embeddings loaded successfully.\")\n",
    "    else:\n",
    "        print(\"Creating new embeddings...\")\n",
    "        texts = [doc.page_content for doc in split_docs]\n",
    "        embedded_docs = embedding_model.embed_documents(texts)\n",
    "\n",
    "        with open(EMBEDDINGS_FILE, 'wb') as f:\n",
    "            pickle.dump(embedded_docs, f)\n",
    "            print(f\"Embeddings saved to {EMBEDDINGS_FILE}\")\n",
    "\n",
    "    return embedded_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store embeddings in Chroma vector store\n",
    "def store_embeddings(split_docs, embedding_model):\n",
    "    vector_store = Chroma.from_documents(split_docs, embedding_model) \n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getting_retriever(llm,vector_store):\n",
    "    \"\"\"Opiton 01: Creating ContextualCompressionRetriever\n",
    "    Contextual Compression will find the relevant records and only contains the relevant data from chunks instead of whole chunks\n",
    "    Maximum Marginal Relevance (mmr) is used to get diverse set of documents.\n",
    "    Option 02: SelfQueryRetrieval for filtering based on sources\"\"\"\n",
    "    # Option 01\n",
    "    compressor = LLMChainExtractor.from_llm(llm)\n",
    "    compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=vector_store.as_retriever(search_kwargs={\"k\": 10})\n",
    "    #search_type = \"mmr\"\n",
    "    )\n",
    "    return compression_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LLM\n",
    "def initialize_llm(model_name=\"llama-3.1-70b-versatile\", temperature=0):\n",
    "    llm = ChatGroq(\n",
    "        model= model_name,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load your document\n",
    "documents = load_documents('resources/9thComputerScience_cleaned.txt')\n",
    "\n",
    "# Split the text into smaller chunks\n",
    "text_splitter = CustomTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "split_docs = text_splitter.split_text(documents[0].page_content)\n",
    "# print(split_docs[10])\n",
    "\n",
    "# Create embeddings and store them\n",
    "# embedded_docs = embed_documents(split_docs, embedding_model)\n",
    "vector_store = store_embeddings(split_docs, embedding_model)\n",
    "llm = initialize_llm()\n",
    "\n",
    "# Create the retriever having contextual compression\n",
    "retriever = getting_retriever(llm,vector_store)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just for testing the normal chain without chat history\n",
    "system_prompt = (\n",
    "    \"Act as a conversational assistant similar to ChatGPT. Engage in natural dialogue and answer questions based on the context provided through the chat history or retrieved using Retrieval-Augmented Generation (RAG). If the relevant context is not found either in the conversation or via RAG, respond by stating that the information is unavailable or ask for more clarification from the user. Do not provide speculative or out-of-context information. Always ensure responses are precise and contextually relevant.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = rag_chain.invoke({\"input\": \"can you give me an interesting topic from 1st chapter\"})\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = rag_chain.invoke({\"input\": \"guide me in each step further\"})\n",
    "# result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Chat history for context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextualize_q_system_prompt = (\n",
    "    \"Act as a conversational assistant similar to ChatGPT. Engage in natural dialogue and answer questions based on the context provided through the chat history or retrieved using Retrieval-Augmented Generation (RAG). If the relevant context is not found either in the conversation or via RAG, respond by stating that the information is unavailable or ask for more clarification from the user. Do not provide speculative or out-of-context information. Always ensure responses are precise and contextually relevant.\"\n",
    ")\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Creating a new retirever that is aware of the chat history. Rest of the things are same.\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = (\n",
    "    \"Act as a conversational assistant similar to ChatGPT. Engage in natural dialogue and answer questions based on the context provided through the chat history or retrieved using Retrieval-Augmented Generation (RAG). If the relevant context is not found either in the conversation or via RAG, respond by stating that the information is unavailable or ask for more clarification from the user. Do not provide speculative or out-of-context information. Always ensure responses are precise and contextually relevant.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat History Working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []   # List of messages in the chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Result Object:  {'input': 'Divide and conquer', 'chat_history': [HumanMessage(content='Divide and conquer', additional_kwargs={}, response_metadata={}), AIMessage(content=\"It seems like you're referring to the Divide and Conquer strategy. This is a problem-solving approach that involves breaking down a complex problem into smaller, more manageable sub-problems. Each sub-problem is then solved individually, and the solutions are combined to form the final solution to the original problem.\\n\\nCan you tell me more about the context in which you're interested in applying the Divide and Conquer strategy? Are you working on a specific problem or project?\", additional_kwargs={}, response_metadata={})], 'context': [Document(metadata={'chapter': 5, 'subtopic': 33}, page_content='e Divide and Conquer: This strategy divides a complex problem into smaller problems.'), Document(metadata={'chapter': 1, 'subtopic': 6}, page_content='e Divide and Conquer: This strategy divides a complex problem into smaller problems.')], 'answer': \"It seems like you're referring to the Divide and Conquer strategy. This is a problem-solving approach that involves breaking down a complex problem into smaller, more manageable sub-problems. Each sub-problem is then solved individually, and the solutions are combined to form the final solution to the original problem.\\n\\nCan you tell me more about the context in which you're interested in applying the Divide and Conquer strategy? Are you working on a specific problem or project?\"}\n",
      "-----------------------------------\n",
      "Answer: It seems like you're referring to the Divide and Conquer strategy. This is a problem-solving approach that involves breaking down a complex problem into smaller, more manageable sub-problems. Each sub-problem is then solved individually, and the solutions are combined to form the final solution to the original problem.\n",
      "\n",
      "Can you tell me more about the context in which you're interested in applying the Divide and Conquer strategy? Are you working on a specific problem or project?\n",
      "-----------------------------------\n",
      "Context: [Document(metadata={'chapter': 5, 'subtopic': 33}, page_content='e Divide and Conquer: This strategy divides a complex problem into smaller problems.'), Document(metadata={'chapter': 1, 'subtopic': 6}, page_content='e Divide and Conquer: This strategy divides a complex problem into smaller problems.')]\n",
      "-----------------------------------\n",
      "Chat History: [HumanMessage(content='Divide and conquer', additional_kwargs={}, response_metadata={}), AIMessage(content=\"It seems like you're referring to the Divide and Conquer strategy. This is a problem-solving approach that involves breaking down a complex problem into smaller, more manageable sub-problems. Each sub-problem is then solved individually, and the solutions are combined to form the final solution to the original problem.\\n\\nCan you tell me more about the context in which you're interested in applying the Divide and Conquer strategy? Are you working on a specific problem or project?\", additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "query = \"Divide and conquer\"\n",
    "result = rag_chain.invoke({\"input\": query,\"chat_history\": chat_history})\n",
    "# Append the user query and the system response to the chat history\n",
    "chat_history.extend(\n",
    "    [\n",
    "        HumanMessage(content=query),\n",
    "        AIMessage(content=result[\"answer\"]),\n",
    "    ]\n",
    ")\n",
    "print(\"Overall Result Object: \",result)\n",
    "print(\"-----------------------------------\")\n",
    "print(\"Answer:\",result[\"answer\"])\n",
    "print(\"-----------------------------------\")\n",
    "print(\"Context:\",result[\"context\"])\n",
    "print(\"-----------------------------------\")\n",
    "print(\"Chat History:\",chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "page_content='e Divide and Conquer: This strategy divides a complex problem into smaller problems.' metadata={'chapter': 5, 'subtopic': 33}\n",
      "page_content='e Divide and Conquer: This strategy divides a complex problem into smaller problems.' metadata={'chapter': 1, 'subtopic': 6}\n"
     ]
    }
   ],
   "source": [
    "print(len(result['context']))\n",
    "\n",
    "for item in result['context']:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It seems like you're referring to the Divide and Conquer strategy. This is a problem-solving approach that involves breaking down a complex problem into smaller, more manageable sub-problems. Each sub-problem is then solved individually, and the solutions are combined to form the final solution to the original problem.\n",
      "\n",
      "Can you tell me more about the context in which you're interested in applying the Divide and Conquer strategy? Are you working on a specific problem or project?\n"
     ]
    }
   ],
   "source": [
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Follow up question without mentioning any particular term to test the memory\n",
    "# query = \"chemistry\"\n",
    "# result = rag_chain.invoke({\"input\": query,\"chat_history\": chat_history})\n",
    "# chat_history.extend(\n",
    "#     [\n",
    "#         HumanMessage(content=query),\n",
    "#         AIMessage(content=result[\"answer\"]),\n",
    "#     ]\n",
    "# )\n",
    "# print(\"Overall Result Object: \",result)\n",
    "# print(\"-----------------------------------\")\n",
    "# print(\"Answer:\",result[\"answer\"])\n",
    "# print(\"-----------------------------------\")\n",
    "# print(\"Context:\",result[\"context\"])\n",
    "# print(\"-----------------------------------\")\n",
    "# print(\"Chat History:\",chat_history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
